{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd82bda-da93-4dbe-a77d-4383c048d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to the fine-tuned model checkpoint\n",
    "checkpoint_path = \"/opt/jupyter/whisper-tiny-az/checkpoint-4000\"  # Your checkpoint directory\n",
    "processor_checkpoint_path =\"/opt/jupyter/whisper-tiny-az\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a09178-ebd3-4086-bef9-ee1f3e787487",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(processor_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c19787-eba6-4894-875c-6aae947b6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "098dbbef-9f32-43b1-8488-a2922f32106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Sadece birkaç gün.\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(audio_path, target_sampling_rate=16000):\n",
    "    # Validate audio file\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample audio to 16000 Hz if necessary\n",
    "    if sr != target_sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sampling_rate)\n",
    "        audio = resampler(audio)\n",
    "    \n",
    "    # Ensure audio is mono (Whisper expects single-channel audio)\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "    \n",
    "    # Process audio for Whisper model\n",
    "    input_features = processor(audio[0].numpy(), sampling_rate=target_sampling_rate, return_tensors='pt').input_features.to(device)\n",
    "    \n",
    "    # Generate transcription\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "# Example usage\n",
    "file = \"/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856093.mp3\"\n",
    "try:\n",
    "    transcription = transcribe_audio(file)\n",
    "    print('Transcription:', transcription)\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5ef6a8e-68e2-4f52-b6fd-3061d0642f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sadece birkaç gün.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transcription with attention mask\n",
    "audio_path = \"/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856093.mp3\"\n",
    "target_sampling_rate = 16000\n",
    "# Load audio\n",
    "audio, sr = torchaudio.load(audio_path)\n",
    "\n",
    "# Resample audio to 16000 Hz if necessary\n",
    "if sr != target_sampling_rate:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sampling_rate)\n",
    "    audio = resampler(audio)\n",
    "    # Ensure audio is mono (Whisper expects single-channel audio)\n",
    "if audio.shape[0] > 1:\n",
    "    audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "# Process audio with attention mask\n",
    "processed_audio = processor(\n",
    "    audio[0].numpy(),\n",
    "    sampling_rate=16000,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True  # Include attention mask\n",
    ")\n",
    "input_features = processed_audio.input_features.to(device)\n",
    "attention_mask = processed_audio.attention_mask.to(device)  # Get attention mask\n",
    "    \n",
    "# Generate transcription with attention mask\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    predicted_ids = model.generate(\n",
    "        input_features,\n",
    "        attention_mask=attention_mask,  # Pass attention mask to model\n",
    "        max_length=225,\n",
    "        num_beams=4,  # Beam search for better results\n",
    "        return_dict_in_generate=False  # Ensure simple tensor output\n",
    "    )\n",
    "\n",
    "# Decode transcription\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db01c7b-dd63-47e4-92fc-3ec81ee4cef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
