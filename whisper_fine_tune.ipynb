{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1c2327-2298-4bba-a30f-71becf5cfc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9785c4231354a6cac47b6a5f0478431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8731dfa-1e42-4be9-8671-517015aac854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5603afa2a594ddea5d386edb9d51a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 25998it [00:00, 296422.64it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c94fa4f6d83458ca59978d8c8fb94f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 10127it [00:00, 311232.13it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a87e42ff9164f4e9a5b53e60748b4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 10143it [00:00, 316312.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e29959c9078426b98fb1b2b63a3e69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 151it [00:00, 227492.78it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce3239a13fd4357a630e0ab2756c256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 3593it [00:00, 305428.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"tr\", split=\"train\")\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"tr\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632670e-e078-4843-b348-2083edb42542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 25998\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 10143\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edce47bd-7d18-42c4-87c0-f4534acaa778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 25998\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 10143\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c563865-d657-40e5-ba8f-c9172d2eb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ac3dd9-7e51-4e25-be22-0fac1dcc89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"azerbaijani\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd2511d-4556-4bdc-a324-9fb48abe9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"azerbaijani\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d71b67-71aa-418a-ba4a-c74700f2c00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856093.mp3', 'array': array([ 1.42108547e-14, -7.10542736e-14, -1.98951966e-13, ...,\n",
      "       -9.25481118e-07,  6.68777466e-06,  5.80005053e-06], shape=(157248,)), 'sampling_rate': 48000}, 'sentence': 'Sadece birkaç gün.'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f940af-635a-4011-b5ba-88d64b78f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "356b0724-92c7-474b-9435-fc159ef2a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856093.mp3', 'array': array([-1.45519152e-11, -7.27595761e-12, -1.81898940e-12, ...,\n",
      "        1.13121732e-05, -1.24729513e-06, -1.83326620e-06], shape=(52416,)), 'sampling_rate': 16000}, 'sentence': 'Sadece birkaç gün.'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee586e2c-685c-4df5-aad2-19b043a76aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856095.mp3', 'array': array([ 4.54747351e-12, -1.81898940e-12, -5.45696821e-12, ...,\n",
      "        1.26162095e-05,  3.88094231e-05,  8.69208179e-06], shape=(58176,)), 'sampling_rate': 16000}, 'sentence': 'Çok kolaydır.'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b102454-f1e3-4e93-88f5-a52ba65224f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadece birkaç gün.\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "935fa6dc-65e3-44bc-a903-fdaba6168c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50258, 50304, 50359, 50363, 50, 25978, 1904, 2330, 1138, 14472, 13, 50257]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(common_voice[\"train\"][0]['sentence']).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13bef1ce-2a54-480c-8587-08d6e46eaf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4601af0a-515b-4f5a-babb-9a573408f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_pre = common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88e228b-f61a-4079-a5cf-a20704295b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84edf4e9f6d94328936da70b771466ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d538e34dea548c286c3fa56d6032f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice_pre = common_voice_pre.map(prepare_dataset, remove_columns=common_voice_pre.column_names[\"train\"], num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6f3d57e-e6af-432e-8ca7-58a17511b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 25998\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 10143\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173cf7dc-1959-4325-8b64-6748e7bae7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 25998\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_pre['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3579c0b-4cb7-42eb-a4f4-c6b61798df61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50258, 50304, 50359, 50363, 50, 25978, 1904, 2330, 1138, 14472, 13, 50257]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_pre['train']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0432621-a819-4596-9f78-22290231af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e1ed33-fba2-4a4b-93af-eb9cb93fe427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"azerbaijani\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e26ad5dc-23a4-40b1-90b7-2397135bdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1836be84-856a-4deb-b2d0-20d0e8a147e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c905f6-361f-43a7-b314-9d883c1ad7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c35d2523-b926-4634-a4d5-58442f886d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d60c790-ceba-457e-ac15-85c71bee351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./whisper-tiny-az',\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,  # Already enabled to reduce memory usage\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Add this to avoid graph issues\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    eval_strategy='steps',\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=['tensorboard'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='wer',\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b3005f2-173d-4c61-9005-bfd2d5e7f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26274/1369350931.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice_pre[\"train\"],\n",
    "    eval_dataset=common_voice_pre[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d2a84c6-37ed-40e5-a04d-4d12eb79583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc9187c1-ee50-46a5-8a4c-b14c8b6aeb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 8:17:01, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.648022</td>\n",
       "      <td>49.240263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.661262</td>\n",
       "      <td>48.889018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.686802</td>\n",
       "      <td>49.674953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.698702</td>\n",
       "      <td>49.475073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3852: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.24152549946308136, metrics={'train_runtime': 29830.3658, 'train_samples_per_second': 8.582, 'train_steps_per_second': 0.134, 'total_flos': 6.291356092416e+18, 'train_loss': 0.24152549946308136, 'epoch': 9.829028290282903})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ed5e635-a94a-40f0-95fc-ae51d0cf97f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./whisper-tiny-az')\n",
    "processor.save_pretrained('./whisper-tiny-az')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9e44900-de65-46d5-9bf3-cb6aa0f49ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Sadece birkaç gün.\n",
      "Transcription: Çok kolaydır.\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def transcribe_audio(audio_path, target_sampling_rate=16000):\n",
    "    # Load audio\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample audio to 16000 Hz if necessary\n",
    "    if sr != target_sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sampling_rate)\n",
    "        audio = resampler(audio)\n",
    "    \n",
    "    # Ensure audio is mono (Whisper expects single-channel audio)\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "    \n",
    "    # Process audio for Whisper model\n",
    "    input_features = processor(audio[0].numpy(), sampling_rate=target_sampling_rate, return_tensors='pt').input_features.to(device)\n",
    "    \n",
    "    # Generate transcription\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "# Example usage\n",
    "file = \"/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856093.mp3\"\n",
    "file2 = \"/home/user/.cache/huggingface/datasets/downloads/extracted/683c6f874760e64ca0a4107d56d97ef171e5dd8fdbcaf9ac15b5f3df0e085d89/tr_train_0/common_voice_tr_28856095.mp3\"\n",
    "\n",
    "print('Transcription:', transcribe_audio(file))\n",
    "print('Transcription:', transcribe_audio(file2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa3d63-6264-4fc4-98de-83d658df7de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
